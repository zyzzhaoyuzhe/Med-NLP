{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import scipy.sparse\n",
    "import itertools\n",
    "import pandas as pd\n",
    "from utils import *\n",
    "from collections import defaultdict, OrderedDict\n",
    "file = 'Data/upto1526.xlsx'\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "velo_fields = {\n",
    "    'cca': ['common carotid', 'cca'],\n",
    "    'bulb': ['carotid bulb'],\n",
    "    'eca': ['external carotid'],\n",
    "    'ica': ['internal carotid', 'ica']\n",
    "}\n",
    "prefix = {\n",
    "    'p': ['proximal', 'prox'],\n",
    "    'd': ['distal', 'dist'],\n",
    "    'm': ['middle', 'mid'],\n",
    "}\n",
    "fuzzyName = {\n",
    "    'study': ['study', 'examination'],\n",
    "    'findings': ['findings'],\n",
    "    'impression': ['impression'],\n",
    "    'history': ['history', 'indication'],\n",
    "    'comparison': ['comparison'],\n",
    "    'technique': ['technique'],\n",
    "    'signed by': ['signed by'],\n",
    "}\n",
    "orderField = [u'study',\n",
    "              u'history',\n",
    "              u'comparison',\n",
    "              u'technique',\n",
    "              u'findings',\n",
    "              u'impression', \n",
    "              u'signed by',\n",
    "             ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_raw = pd.read_excel(open(file, 'r')) \n",
    "#df_raw = pd.read_excel(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ps = Parser(fuzzyName)\n",
    "ps.parser(df_raw)\n",
    "df = ps.df\n",
    "for idx, row in df['findings'].iteritems():\n",
    "    try:\n",
    "        text, velos = parse_findings(row)\n",
    "        df.set_value(idx, 'findings', text)\n",
    "        for n, v in velos:\n",
    "            df.set_value(idx, n, v)\n",
    "    except:\n",
    "        pass\n",
    "discardField = ['Report Text']\n",
    "foo = [item for item in df.columns.tolist() if item not in orderField+discardField]\n",
    "foo.sort()\n",
    "CORE_COL = orderField + foo\n",
    "df = df[CORE_COL]\n",
    "df = pd.concat([df_raw[['Past', 'Present']], df[CORE_COL]], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#df = null2empty(df, 'history' 'impression')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# write to documents\n",
    "#df[col].to_excel('reports(07192017).xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use findings and impression to predict present and pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1525, 9)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wuxiao/anaconda/envs/ipykernel_py2/lib/python2.7/site-packages/pandas/core/indexing.py:179: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self._setitem_with_indexer(indexer, value)\n"
     ]
    }
   ],
   "source": [
    "df = null2empty(df, ['history','impression','comparison'])\n",
    "print df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#for PAST\n",
    "fields = ['history','impression','findings', 'comparison']\n",
    "msk = np.random.rand(len(df[~df['Past'].isnull() & df['Past'] != 0])) < 0.6\n",
    "df_train = df[~df['Past'].isnull() & df['Past'] != 0][msk]\n",
    "y_train = np.array(df_train['Past'].astype(int))\n",
    "text_train = df2text(df_train, fields=fields)\n",
    "df_test = df[~msk]\n",
    "y_test = np.array(df_test['Past'].astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#for PRESENT\n",
    "fields = ['history','impression','findings', 'comparison']\n",
    "msk1 = np.random.rand(len(df[~df['Present'].isnull() & df['Present'] != 0])) < 0.7\n",
    "df_train = df[~df['Present'].isnull() & df['Present'] != 0][msk1]\n",
    "y_train = np.array(df_train['Present'].astype(int))\n",
    "text_train = df2text(df_train, fields=fields)\n",
    "df_test = df[~df['Present'].isnull() & df['Present'] != 0][~msk1]\n",
    "y_test = np.array(df_test['Present'].astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sixmonth followup of right carotid occlusion   redemonstration of chronic occlusion of the right internal carotid artery  no significant interval change in  stenosis of the left internal carotid artery and highgrade stenosis of the left external carotid artery the stenosis in the left internal carotid artery is likely at the low end of the range and velocities may be slightly overestimating the degree of stenosis due to the contralateral occlusion however on grayscale imaging there appears to be at least a  stenosis analysis of internal carotid stenosis is based on duplex doppler velocity parameters that correlate with the calculation of an internal carotid artery stenosis according to the nascet criteria  right again seen is complete occlusion of the right internal carotid artery diffuse noncalcified plaque along the common carotid artery and carotid bulb is redemonstrated the waveforms in the right common carotid artery are high resistance expected finding given chronic occlusion of the internal carotid artery the peak systolic velocities on the right are as follows proximal common carotid  cms distal common carotid  cms carotid bulb  cms external carotid  cms internal carotid occluded the right vertebral artery is patent and demonstrates normal antegrade flow left diffuse noncalcified plaque along the common carotid artery and mixed calcified and noncalcified plaques in the carotid bulb and proximal internal carotid artery are again seen tardus parvus waveforms are observed of the left carotid bulb and along the left internal carotid artery the peak systolic velocities on the left are as follows proximal common carotid  cms distal common carotid  cms previously  cms carotid bulb  cms external carotid  cms proximal internal carotid  cms previously  cms icacca middle internal carotid  cms previously  cms distal internal carotid  cms previously  cms the left vertebral artery is patent and demonstrates normal antegrade flow'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_train[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1979"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary = defaultdict(int)\n",
    "for text in text_train:\n",
    "    for word in text.split():\n",
    "        dictionary[word] += 1\n",
    "len(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import operator\n",
    "foo = sorted(dictionary.items(), key=operator.itemgetter(1), reverse=True)\n",
    "word2idx = {}\n",
    "idx2word = {}\n",
    "for idx, item in enumerate(foo):\n",
    "    word2idx[item[0]] = idx\n",
    "    idx2word[idx] = item[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['limited', 'extrapolated']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2idx.keys()[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_train = text2data(df2text(df_train, fields=fields), word2idx)\n",
    "data_test = text2data(df2text(df_test, fields=fields), word2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "transform = TfidfTransformer()\n",
    "data_train = transform.fit_transform(data_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# svm\n",
    "#clf = LinearSVC()\n",
    "#clf = SVC()\n",
    "clf = LogisticRegression()\n",
    "\n",
    "\n",
    "clf.fit(data_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9033203125\n",
      "0.847087378641\n"
     ]
    }
   ],
   "source": [
    "print clf.score(data_train, y_train)\n",
    "print clf.score(data_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "734\n",
      "95\n",
      "191\n",
      "4\n",
      "240\n",
      "8\n",
      "109\n",
      "55\n"
     ]
    }
   ],
   "source": [
    "y_train_pred = clf.predict(data_train)\n",
    "y_test_pred = clf.predict(data_test)\n",
    "CM_train = confusion_matrix(y_train, y_train_pred)\n",
    "CM_test = confusion_matrix(y_test, y_test_pred)\n",
    "print CM_train[0][0] #TN\n",
    "print CM_train[1][0] #FN\n",
    "print CM_train[1][1] #TP\n",
    "print CM_train[0][1] #FP\n",
    "\n",
    "print CM_test[0][0] #TN\n",
    "print CM_test[1][0] #FN\n",
    "print CM_test[1][1] #TP\n",
    "print CM_test[0][1] #FP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "931"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat = np.asarray((data_train[:, 0] > 0).todense()) * 2 - 1\n",
    "sum(y_hat.squeeze() - np.array(y_train) != 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "226"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat = np.asarray((data_test[:,0] > 0).todense()) * 2 - 1\n",
    "sum(y_hat.squeeze() - np.array(y_test) != 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "297"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.int64"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(y_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}