{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import pandas as pd\n",
    "import utils\n",
    "from collections import defaultdict, OrderedDict\n",
    "import numpy as np\n",
    "from IPython.display import display\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.sparse import hstack, vstack\n",
    "import tensorflow as tf\n",
    "from nn_models import TextCNN, TextCNN_field_aware, TextRNN, TextRNN_field_aware, TextRNN_attention\n",
    "import data_helpers\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "import pickle\n",
    "import copy\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.metrics import precision_recall_curve, roc_curve, accuracy_score    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "ORDERED_NAMES = [u'study',\n",
    "                 u'history',\n",
    "                 u'comparison',\n",
    "                 u'technique',\n",
    "                 u'findings',\n",
    "                 u'impression', \n",
    "                 u'signed by',\n",
    "                 ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Data Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1220, 5)\n",
      "(307, 5)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-1    1172\n",
       " 1     355\n",
       "Name: Past, dtype: int64"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load\n",
    "df_processed = pickle.load(open('Data/DataFrame_processed.p', 'rb'))\n",
    "\n",
    "TO_PREDICT = 'Past'\n",
    "FIELDS = [\n",
    "    'history',\n",
    "    'findings',\n",
    "    'comparison',\n",
    "    'impression',\n",
    "#     'Grade',\n",
    "#     'Present',\n",
    "#     'Past'\n",
    "]\n",
    "\n",
    "# df_filtered = df_processed[~df_processed[TO_PREDICT].isnull() & (df_processed[TO_PREDICT] != 0)].sample(frac=1, random_state=1)\n",
    "df_filtered = df_processed[~df_processed[TO_PREDICT].isnull()].sample(frac=1, random_state=1)\n",
    "df_filtered = df_filtered[[TO_PREDICT] + FIELDS]\n",
    "\n",
    "df_train = df_filtered.iloc[:1220]\n",
    "y_train = np.array(df_train[TO_PREDICT].astype(int))\n",
    "enc = LabelEncoder()\n",
    "enc.fit(y_train)\n",
    "y_train = enc.transform(y_train)\n",
    "\n",
    "df_test = df_filtered.iloc[1220:]\n",
    "y_test = np.array(df_test[TO_PREDICT].astype(int))\n",
    "y_test = enc.transform(y_test)\n",
    "\n",
    "print(df_train.shape)\n",
    "print(df_test.shape)\n",
    "df_filtered[TO_PREDICT].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "# Neural Nets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Prep**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Past', 'history', 'findings', 'comparison', 'impression'], dtype='object')"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen = [100,\n",
    "          125,\n",
    "          50,\n",
    "          100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zyzzhaoyuzhe/virtualenvs/nlp3/lib/python3.7/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "x_train_text = utils.Dataframe_Proc.df2text(df_train, df_train.columns[1:])\n",
    "word2idx, idx2word = utils.Text_Proc.ngram_vocab_processor(x_train_text, ngram=1, min_count=2)\n",
    "x_train = np.array(utils.Text_Proc.encode_texts(x_train_text, word2idx, maxlen=sum(maxlen)))\n",
    "\n",
    "enc = OneHotEncoder(sparse=False)\n",
    "y_train = enc.fit_transform(y_train[:, None])\n",
    "\n",
    "x_dev_text = utils.Dataframe_Proc.df2text(df_test, df_test.columns[1:])\n",
    "x_dev = np.array(utils.Text_Proc.encode_texts(x_dev_text, word2idx, maxlen=x_train.shape[1]))\n",
    "\n",
    "y_dev = enc.transform(y_test[:, None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1220, 375)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "foo = pickle.load(open(\"Data/DataFrame_nns.p\", \"rb\"))\n",
    "x_train = foo['x_train']\n",
    "x_dev = foo['x_dev']\n",
    "word2idx = foo['word2idx']\n",
    "idx2word = foo['idx2word']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1386"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word2idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NN Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Parameters:\n",
      "ALLOW_SOFT_PLACEMENT=<absl.flags._flag.BooleanFlag object at 0x13c161710>\n",
      "BATCH_SIZE=<absl.flags._flag.Flag object at 0x133586ac8>\n",
      "CHECKPOINT_EVERY=<absl.flags._flag.Flag object at 0x133586eb8>\n",
      "DO_EVAL=<absl.flags._flag.BooleanFlag object at 0x13c161ba8>\n",
      "DO_TRAIN=<absl.flags._flag.BooleanFlag object at 0x13c161be0>\n",
      "DROPOUT_KEEP_PROB=<absl.flags._flag.Flag object at 0x133586860>\n",
      "EMBEDDING_DIM=<absl.flags._flag.Flag object at 0x1335866d8>\n",
      "EVALUATE_EVERY=<absl.flags._flag.Flag object at 0x133586dd8>\n",
      "F=<absl.flags._flag.Flag object at 0x13c161208>\n",
      "FILTER_SIZES=<absl.flags._flag.Flag object at 0x1335867f0>\n",
      "HIDDEN_SIZE=<absl.flags._flag.Flag object at 0x133586be0>\n",
      "INIT_CHECKPOINT=<absl.flags._flag.Flag object at 0x13c161e10>\n",
      "L2_REG_LAMBDA=<absl.flags._flag.Flag object at 0x133586710>\n",
      "LEARNING_RATE=<absl.flags._flag.Flag object at 0x133586cc0>\n",
      "LOG_DEVICE_PLACEMENT=<absl.flags._flag.BooleanFlag object at 0x13c161f60>\n",
      "LOG_EVERY=<absl.flags._flag.Flag object at 0x13c161a20>\n",
      "NUM_CHECKPOINTS=<absl.flags._flag.Flag object at 0x13c1617f0>\n",
      "NUM_EPOCHS=<absl.flags._flag.Flag object at 0x133586d68>\n",
      "NUM_FILTERS=<absl.flags._flag.Flag object at 0x1335868d0>\n",
      "NUM_LAYERS=<absl.flags._flag.Flag object at 0x1335869b0>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Clear flags\n",
    "if \"FLAGS\" in locals():\n",
    "    for key in [key for key in FLAGS._flags().keys()]:\n",
    "        FLAGS.__delattr__(key)\n",
    "\n",
    "# Both CNN and RNN\n",
    "tf.flags.DEFINE_integer(\"embedding_dim\", 64, \"Dimensionality of character embedding (default: 128)\")\n",
    "tf.flags.DEFINE_float(\"dropout_keep_prob\", 0.5, \"Dropout keep probability (default: 0.5)\")\n",
    "tf.flags.DEFINE_float(\"l2_reg_lambda\", 0.1, \"L2 regularization lambda (default: 0.0)\")\n",
    "\n",
    "# CNN parameter\n",
    "tf.flags.DEFINE_string(\"filter_sizes\", \"3,4\", \"Comma-separated filter sizes (default: '3,4,5')\")\n",
    "tf.flags.DEFINE_integer(\"num_filters\", 128, \"Number of filters per filter size (default: 128)\")\n",
    "\n",
    "# RNN parameter\n",
    "tf.flags.DEFINE_integer('hidden_size', 128, 'Hidden size of LSTM')\n",
    "tf.flags.DEFINE_integer('num_layers', 1, 'Number of LSTM layers')\n",
    "\n",
    "# Training parameters\n",
    "tf.flags.DEFINE_integer(\"batch_size\", 64, \"Batch Size (default: 64)\")\n",
    "tf.flags.DEFINE_float(\"learning_rate\", 1e-3, \"learning rate\")\n",
    "tf.flags.DEFINE_integer(\"num_epochs\", 100, \"Number of training epochs (default: 200)\")\n",
    "tf.flags.DEFINE_integer(\"evaluate_every\", 100, \"Evaluate model on dev set after this many steps (default: 100)\")\n",
    "tf.flags.DEFINE_integer(\"checkpoint_every\", 100, \"Save model after this many steps (default: 100)\")\n",
    "tf.flags.DEFINE_integer(\"num_checkpoints\", 0, \"Number of checkpoints to store (default: 5)\")\n",
    "tf.flags.DEFINE_integer(\"log_every\", 10, \"Logs model on dev set after this many steps (default: 100)\")\n",
    "\n",
    "# Control\n",
    "tf.flags.DEFINE_string(\"init_checkpoint\", \n",
    "                       \"runs/CNN-Field-Aware-Past-1700/checkpoints/model-1700\",\n",
    "                       \"Checkpoint.\")\n",
    "tf.flags.DEFINE_boolean(\"do_train\", False, \"Do training\")\n",
    "tf.flags.DEFINE_boolean(\"do_eval\", True, \"Do evaluation\")\n",
    "\n",
    "\n",
    "# Misc Parameters\n",
    "tf.flags.DEFINE_boolean(\"allow_soft_placement\", True, \"Allow device soft device placement\")\n",
    "tf.flags.DEFINE_boolean(\"log_device_placement\", False, \"Log placement of ops on devices\")\n",
    "\n",
    "# Fix Bugs \n",
    "tf.app.flags.DEFINE_string('f', '', 'kernel')\n",
    "\n",
    "FLAGS = tf.flags.FLAGS\n",
    "# FLAGS._parse_flags()\n",
    "print(\"\\nParameters:\")\n",
    "for attr, value in sorted(FLAGS.__flags.items()):\n",
    "    print(\"{}={}\".format(attr.upper(), value))\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name embedding/Variable:0/grad/hist is illegal; using embedding/Variable_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name embedding/Variable:0/grad/sparsity is illegal; using embedding/Variable_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name CNN_0/conv-maxpool-3/W:0/grad/hist is illegal; using CNN_0/conv-maxpool-3/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name CNN_0/conv-maxpool-3/W:0/grad/sparsity is illegal; using CNN_0/conv-maxpool-3/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name CNN_0/conv-maxpool-3/b:0/grad/hist is illegal; using CNN_0/conv-maxpool-3/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name CNN_0/conv-maxpool-3/b:0/grad/sparsity is illegal; using CNN_0/conv-maxpool-3/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name CNN_0/conv-maxpool-4/W:0/grad/hist is illegal; using CNN_0/conv-maxpool-4/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name CNN_0/conv-maxpool-4/W:0/grad/sparsity is illegal; using CNN_0/conv-maxpool-4/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name CNN_0/conv-maxpool-4/b:0/grad/hist is illegal; using CNN_0/conv-maxpool-4/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name CNN_0/conv-maxpool-4/b:0/grad/sparsity is illegal; using CNN_0/conv-maxpool-4/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name CNN_1/conv-maxpool-3/W:0/grad/hist is illegal; using CNN_1/conv-maxpool-3/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name CNN_1/conv-maxpool-3/W:0/grad/sparsity is illegal; using CNN_1/conv-maxpool-3/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name CNN_1/conv-maxpool-3/b:0/grad/hist is illegal; using CNN_1/conv-maxpool-3/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name CNN_1/conv-maxpool-3/b:0/grad/sparsity is illegal; using CNN_1/conv-maxpool-3/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name CNN_1/conv-maxpool-4/W:0/grad/hist is illegal; using CNN_1/conv-maxpool-4/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name CNN_1/conv-maxpool-4/W:0/grad/sparsity is illegal; using CNN_1/conv-maxpool-4/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name CNN_1/conv-maxpool-4/b:0/grad/hist is illegal; using CNN_1/conv-maxpool-4/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name CNN_1/conv-maxpool-4/b:0/grad/sparsity is illegal; using CNN_1/conv-maxpool-4/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name CNN_2/conv-maxpool-3/W:0/grad/hist is illegal; using CNN_2/conv-maxpool-3/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name CNN_2/conv-maxpool-3/W:0/grad/sparsity is illegal; using CNN_2/conv-maxpool-3/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name CNN_2/conv-maxpool-3/b:0/grad/hist is illegal; using CNN_2/conv-maxpool-3/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name CNN_2/conv-maxpool-3/b:0/grad/sparsity is illegal; using CNN_2/conv-maxpool-3/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name CNN_2/conv-maxpool-4/W:0/grad/hist is illegal; using CNN_2/conv-maxpool-4/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name CNN_2/conv-maxpool-4/W:0/grad/sparsity is illegal; using CNN_2/conv-maxpool-4/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name CNN_2/conv-maxpool-4/b:0/grad/hist is illegal; using CNN_2/conv-maxpool-4/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name CNN_2/conv-maxpool-4/b:0/grad/sparsity is illegal; using CNN_2/conv-maxpool-4/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name CNN_3/conv-maxpool-3/W:0/grad/hist is illegal; using CNN_3/conv-maxpool-3/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name CNN_3/conv-maxpool-3/W:0/grad/sparsity is illegal; using CNN_3/conv-maxpool-3/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name CNN_3/conv-maxpool-3/b:0/grad/hist is illegal; using CNN_3/conv-maxpool-3/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name CNN_3/conv-maxpool-3/b:0/grad/sparsity is illegal; using CNN_3/conv-maxpool-3/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name CNN_3/conv-maxpool-4/W:0/grad/hist is illegal; using CNN_3/conv-maxpool-4/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name CNN_3/conv-maxpool-4/W:0/grad/sparsity is illegal; using CNN_3/conv-maxpool-4/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name CNN_3/conv-maxpool-4/b:0/grad/hist is illegal; using CNN_3/conv-maxpool-4/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name CNN_3/conv-maxpool-4/b:0/grad/sparsity is illegal; using CNN_3/conv-maxpool-4/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name W:0/grad/hist is illegal; using W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name W:0/grad/sparsity is illegal; using W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name output/b:0/grad/hist is illegal; using output/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name output/b:0/grad/sparsity is illegal; using output/b_0/grad/sparsity instead.\n",
      "Writing to /Users/zyzzhaoyuzhe/Documents/Med-NLP/runs/CNN_64_128_1553447439\n",
      "\n",
      "[<tf.Variable 'embedding/Variable:0' shape=(1386, 64) dtype=float32_ref>, <tf.Variable 'CNN_0/conv-maxpool-3/W:0' shape=(3, 64, 1, 128) dtype=float32_ref>, <tf.Variable 'CNN_0/conv-maxpool-3/b:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'CNN_0/conv-maxpool-4/W:0' shape=(4, 64, 1, 128) dtype=float32_ref>, <tf.Variable 'CNN_0/conv-maxpool-4/b:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'CNN_1/conv-maxpool-3/W:0' shape=(3, 64, 1, 128) dtype=float32_ref>, <tf.Variable 'CNN_1/conv-maxpool-3/b:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'CNN_1/conv-maxpool-4/W:0' shape=(4, 64, 1, 128) dtype=float32_ref>, <tf.Variable 'CNN_1/conv-maxpool-4/b:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'CNN_2/conv-maxpool-3/W:0' shape=(3, 64, 1, 128) dtype=float32_ref>, <tf.Variable 'CNN_2/conv-maxpool-3/b:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'CNN_2/conv-maxpool-4/W:0' shape=(4, 64, 1, 128) dtype=float32_ref>, <tf.Variable 'CNN_2/conv-maxpool-4/b:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'CNN_3/conv-maxpool-3/W:0' shape=(3, 64, 1, 128) dtype=float32_ref>, <tf.Variable 'CNN_3/conv-maxpool-3/b:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'CNN_3/conv-maxpool-4/W:0' shape=(4, 64, 1, 128) dtype=float32_ref>, <tf.Variable 'CNN_3/conv-maxpool-4/b:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'W:0' shape=(1024, 2) dtype=float32_ref>, <tf.Variable 'output/b:0' shape=(2,) dtype=float32_ref>, <tf.Variable 'global_step:0' shape=() dtype=int32_ref>, <tf.Variable 'beta1_power:0' shape=() dtype=float32_ref>, <tf.Variable 'beta2_power:0' shape=() dtype=float32_ref>, <tf.Variable 'embedding/Variable/Adam:0' shape=(1386, 64) dtype=float32_ref>, <tf.Variable 'embedding/Variable/Adam_1:0' shape=(1386, 64) dtype=float32_ref>, <tf.Variable 'CNN_0/conv-maxpool-3/W/Adam:0' shape=(3, 64, 1, 128) dtype=float32_ref>, <tf.Variable 'CNN_0/conv-maxpool-3/W/Adam_1:0' shape=(3, 64, 1, 128) dtype=float32_ref>, <tf.Variable 'CNN_0/conv-maxpool-3/b/Adam:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'CNN_0/conv-maxpool-3/b/Adam_1:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'CNN_0/conv-maxpool-4/W/Adam:0' shape=(4, 64, 1, 128) dtype=float32_ref>, <tf.Variable 'CNN_0/conv-maxpool-4/W/Adam_1:0' shape=(4, 64, 1, 128) dtype=float32_ref>, <tf.Variable 'CNN_0/conv-maxpool-4/b/Adam:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'CNN_0/conv-maxpool-4/b/Adam_1:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'CNN_1/conv-maxpool-3/W/Adam:0' shape=(3, 64, 1, 128) dtype=float32_ref>, <tf.Variable 'CNN_1/conv-maxpool-3/W/Adam_1:0' shape=(3, 64, 1, 128) dtype=float32_ref>, <tf.Variable 'CNN_1/conv-maxpool-3/b/Adam:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'CNN_1/conv-maxpool-3/b/Adam_1:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'CNN_1/conv-maxpool-4/W/Adam:0' shape=(4, 64, 1, 128) dtype=float32_ref>, <tf.Variable 'CNN_1/conv-maxpool-4/W/Adam_1:0' shape=(4, 64, 1, 128) dtype=float32_ref>, <tf.Variable 'CNN_1/conv-maxpool-4/b/Adam:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'CNN_1/conv-maxpool-4/b/Adam_1:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'CNN_2/conv-maxpool-3/W/Adam:0' shape=(3, 64, 1, 128) dtype=float32_ref>, <tf.Variable 'CNN_2/conv-maxpool-3/W/Adam_1:0' shape=(3, 64, 1, 128) dtype=float32_ref>, <tf.Variable 'CNN_2/conv-maxpool-3/b/Adam:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'CNN_2/conv-maxpool-3/b/Adam_1:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'CNN_2/conv-maxpool-4/W/Adam:0' shape=(4, 64, 1, 128) dtype=float32_ref>, <tf.Variable 'CNN_2/conv-maxpool-4/W/Adam_1:0' shape=(4, 64, 1, 128) dtype=float32_ref>, <tf.Variable 'CNN_2/conv-maxpool-4/b/Adam:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'CNN_2/conv-maxpool-4/b/Adam_1:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'CNN_3/conv-maxpool-3/W/Adam:0' shape=(3, 64, 1, 128) dtype=float32_ref>, <tf.Variable 'CNN_3/conv-maxpool-3/W/Adam_1:0' shape=(3, 64, 1, 128) dtype=float32_ref>, <tf.Variable 'CNN_3/conv-maxpool-3/b/Adam:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'CNN_3/conv-maxpool-3/b/Adam_1:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'CNN_3/conv-maxpool-4/W/Adam:0' shape=(4, 64, 1, 128) dtype=float32_ref>, <tf.Variable 'CNN_3/conv-maxpool-4/W/Adam_1:0' shape=(4, 64, 1, 128) dtype=float32_ref>, <tf.Variable 'CNN_3/conv-maxpool-4/b/Adam:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'CNN_3/conv-maxpool-4/b/Adam_1:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'W/Adam:0' shape=(1024, 2) dtype=float32_ref>, <tf.Variable 'W/Adam_1:0' shape=(1024, 2) dtype=float32_ref>, <tf.Variable 'output/b/Adam:0' shape=(2,) dtype=float32_ref>, <tf.Variable 'output/b/Adam_1:0' shape=(2,) dtype=float32_ref>]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from runs/CNN-Field-Aware-Past-1700/checkpoints/model-1700\n"
     ]
    }
   ],
   "source": [
    "with tf.Graph().as_default():\n",
    "    session_conf = tf.ConfigProto(\n",
    "      allow_soft_placement=FLAGS.allow_soft_placement,\n",
    "      log_device_placement=FLAGS.log_device_placement)\n",
    "    sess = tf.Session(config=session_conf)\n",
    "    with sess.as_default():\n",
    "        \n",
    "#         model = TextCNN(\n",
    "#             sequence_length=x_train.shape[1],\n",
    "#             num_classes=y_train.shape[1],\n",
    "#             vocab_size=len(word2idx),\n",
    "#             embedding_size=FLAGS.embedding_dim,\n",
    "#             filter_sizes=list(map(int, FLAGS.filter_sizes.split(\",\"))),\n",
    "#             num_filters=FLAGS.num_filters,\n",
    "#             l2_reg_lambda=FLAGS.l2_reg_lambda)\n",
    "\n",
    "        model = TextCNN_field_aware(sequence_lengths=maxlen,\n",
    "                                    num_classes=y_train.shape[1],\n",
    "                                    vocab_size=len(word2idx),\n",
    "                                    embedding_size=FLAGS.embedding_dim,\n",
    "                                    filter_sizes=list(map(int, FLAGS.filter_sizes.split(\",\"))),\n",
    "                                    num_filters=FLAGS.num_filters,\n",
    "                                    l2_reg_lambda=FLAGS.l2_reg_lambda)\n",
    "\n",
    "#         model = TextRNN(sequence_length=sum(maxlen),\n",
    "#                         num_classes=y_train.shape[1],\n",
    "#                         vocab_size=len(word2idx),\n",
    "#                         embedding_size=FLAGS.embedding_dim,\n",
    "#                         hidden_size=FLAGS.hidden_size,\n",
    "#                         num_layers=FLAGS.num_layers,\n",
    "#                         l2_reg_lambda=FLAGS.l2_reg_lambda)\n",
    "\n",
    "#         model = TextRNN_field_aware(sequence_lengths=maxlen,\n",
    "#                                     num_classes=y_train.shape[1],\n",
    "#                                     vocab_size=len(word2idx),\n",
    "#                                     embedding_size=FLAGS.embedding_dim,\n",
    "#                                     hidden_size=FLAGS.hidden_size,\n",
    "#                                     num_layers=FLAGS.num_layers,\n",
    "#                                     l2_reg_lambda=FLAGS.l2_reg_lambda)\n",
    "\n",
    "#         model = TextRNN_attention(sequence_length=sum(maxlen),\n",
    "#                         num_classes=y_train.shape[1],\n",
    "#                         vocab_size=len(word2idx),\n",
    "#                         embedding_size=FLAGS.embedding_dim,\n",
    "#                         hidden_size=FLAGS.hidden_size,\n",
    "#                         num_layers=FLAGS.num_layers,\n",
    "#                         l2_reg_lambda=FLAGS.l2_reg_lambda)\n",
    "\n",
    "\n",
    "        # Define Training procedure\n",
    "        global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "        # optimizer = tf.train.GradientDescentOptimizer(1e-3)\n",
    "        optimizer = tf.train.AdamOptimizer(FLAGS.learning_rate)\n",
    "        grads_and_vars = optimizer.compute_gradients(model.loss)\n",
    "        train_op = optimizer.apply_gradients(grads_and_vars, global_step=global_step)\n",
    "\n",
    "        # Keep track of gradient values and sparsity (optional)\n",
    "        grad_summaries = []\n",
    "        for g, v in grads_and_vars:\n",
    "            if g is not None:\n",
    "                grad_hist_summary = tf.summary.histogram(\"{}/grad/hist\".format(v.name), g)\n",
    "                sparsity_summary = tf.summary.scalar(\"{}/grad/sparsity\".format(v.name), tf.nn.zero_fraction(g))\n",
    "                grad_summaries.append(grad_hist_summary)\n",
    "                grad_summaries.append(sparsity_summary)\n",
    "        grad_summaries_merged = tf.summary.merge(grad_summaries)\n",
    "\n",
    "        # Output directory for models and summaries\n",
    "        timestamp = str(int(time.time()))\n",
    "\n",
    "        if 'CNN' in model.__class__.__name__:\n",
    "            name = \"CNN_{}_{}_\".format(FLAGS.embedding_dim, FLAGS.num_filters)\n",
    "        elif 'RNN' in model.__class__.__name__:\n",
    "            name = \"RNN_{}_{}_{}_\".format(FLAGS.embedding_dim, FLAGS.hidden_size, FLAGS.num_layers)\n",
    "\n",
    "\n",
    "        out_dir = os.path.abspath(os.path.join(os.path.curdir, \"runs\", name + timestamp))\n",
    "        print(\"Writing to {}\\n\".format(out_dir))\n",
    "\n",
    "        # Summaries for loss and accuracy\n",
    "        loss_summary = tf.summary.scalar(\"loss\", model.loss)\n",
    "        acc_summary = tf.summary.scalar(\"accuracy\", model.accuracy)\n",
    "\n",
    "        # Train Summaries\n",
    "        train_summary_op = tf.summary.merge([loss_summary, acc_summary, grad_summaries_merged])\n",
    "        train_summary_dir = os.path.join(out_dir, \"summaries\", \"train\")\n",
    "        train_summary_writer = tf.summary.FileWriter(train_summary_dir, sess.graph)\n",
    "\n",
    "        # Dev summaries\n",
    "        dev_summary_op = tf.summary.merge([loss_summary, acc_summary])\n",
    "        dev_summary_dir = os.path.join(out_dir, \"summaries\", \"dev\")\n",
    "        dev_summary_writer = tf.summary.FileWriter(dev_summary_dir, sess.graph)\n",
    "\n",
    "        # Checkpoint directory. Tensorflow assumes this directory already exists so we need to create it\n",
    "        checkpoint_dir = os.path.abspath(os.path.join(out_dir, \"checkpoints\"))\n",
    "        checkpoint_prefix = os.path.join(checkpoint_dir, \"model\")\n",
    "        if not os.path.exists(checkpoint_dir):\n",
    "            os.makedirs(checkpoint_dir)\n",
    "        print(tf.global_variables())\n",
    "        saver = tf.train.Saver(tf.global_variables(), max_to_keep=FLAGS.num_checkpoints)\n",
    "        \n",
    "        # Initialize from checkpoint\n",
    "        if FLAGS.init_checkpoint:\n",
    "            restorer = tf.train.Saver(tf.global_variables())\n",
    "            restorer.restore(sess, FLAGS.init_checkpoint)\n",
    "        else:\n",
    "            # Initialize all variables\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            \n",
    "\n",
    "        # Write vocabulary\n",
    "        # vocab_processor.save(os.path.join(out_dir, \"vocab\"))\n",
    "        \n",
    "        def train_step(x_batch, y_batch):\n",
    "            \"\"\"\n",
    "            A single training step\n",
    "            \"\"\"\n",
    "            feed_dict = {\n",
    "                model.input_x: x_batch,\n",
    "                model.input_y: y_batch,\n",
    "                model.dropout_keep_prob: FLAGS.dropout_keep_prob,\n",
    "                model.batch_size: len(x_batch)\n",
    "            }\n",
    "\n",
    "            _, step, summaries, loss, accuracy = sess.run(\n",
    "                [train_op, global_step, train_summary_op, model.loss, model.accuracy],\n",
    "                feed_dict)\n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "            if step % FLAGS.log_every == 0:\n",
    "                print(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, accuracy))\n",
    "            train_summary_writer.add_summary(summaries, step)\n",
    "\n",
    "        def dev_step(x_batch, y_batch, writer=None):\n",
    "            \"\"\"\n",
    "            Evaluates model on a dev set\n",
    "            \"\"\"\n",
    "            feed_dict = {\n",
    "                model.input_x: x_batch,\n",
    "                model.input_y: y_batch,\n",
    "                model.dropout_keep_prob: 1.0,\n",
    "                model.batch_size: len(x_batch)\n",
    "            }\n",
    "\n",
    "            step, summaries, loss, accuracy = sess.run(\n",
    "                [global_step, dev_summary_op, model.loss, model.accuracy],\n",
    "                feed_dict)\n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "            print(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, accuracy))\n",
    "            if writer:\n",
    "                writer.add_summary(summaries, step)\n",
    "\n",
    "        \n",
    "        if FLAGS.do_train:\n",
    "            # Generate batches\n",
    "            batches = data_helpers.batch_iter(\n",
    "                list(zip(x_train, y_train)), FLAGS.batch_size, FLAGS.num_epochs)\n",
    "            # Training loop. For each batch...\n",
    "            for batch in batches:\n",
    "                x_batch, y_batch = zip(*batch)\n",
    "\n",
    "                train_step(x_batch, y_batch)\n",
    "                current_step = tf.train.global_step(sess, global_step)\n",
    "                if current_step % FLAGS.evaluate_every == 0:\n",
    "                    print(\"\\nEvaluation:\")\n",
    "                    dev_step(x_dev, y_dev, writer=dev_summary_writer)\n",
    "                    print(\"\")\n",
    "                if current_step % FLAGS.checkpoint_every == 0:\n",
    "                    path = saver.save(sess, checkpoint_prefix, global_step=current_step)\n",
    "                    print(\"Saved model checkpoint to {}\\n\".format(path))\n",
    "\n",
    "        if FLAGS.do_eval:\n",
    "            feed_dict = {\n",
    "                model.input_x: x_dev,\n",
    "                model.input_y: y_dev,\n",
    "                model.dropout_keep_prob: 1.0,\n",
    "                model.batch_size: len(x_dev)\n",
    "            }\n",
    "            y_dev_pred = sess.run([model.predictions], feed_dict=feed_dict)\n",
    "            y_dev_prob = sess.run([tf.nn.softmax(model.logits, axis=1)], feed_dict=feed_dict)\n",
    "    #         y_dev_pred, alpha = sess.run([model.predictions, model.alpha], feed_dict=feed_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PR Curve\n",
    "pr_curves = []  # reset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "_y_true = y_test\n",
    "_y_pred = y_dev_prob[0][:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zyzzhaoyuzhe/virtualenvs/nlp3/lib/python3.7/site-packages/sklearn/metrics/ranking.py:526: RuntimeWarning: invalid value encountered in true_divide\n",
      "  recall = tps / tps[-1]\n"
     ]
    }
   ],
   "source": [
    "pr_curves.append(precision_recall_curve(_y_true, _y_pred, pos_label=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for pr_curve in pr_curves[-3:]:\n",
    "    plt.plot(pr_curve[1], pr_curve[0])\n",
    "plt.legend([\"RNN-Attention\", \"CNN\", \"Logistic Regression\"],fontsize=14)\n",
    "plt.xlabel('Recall', fontsize=16)\n",
    "plt.ylabel('Precision', fontsize=16)\n",
    "plt.savefig('images/predict_past_pr_curve.pdf', dpi=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6989311\n",
      "0.0 nan\n",
      "0.7899159663865546\n"
     ]
    }
   ],
   "source": [
    "precision, recall, thres = pr_curves[0]\n",
    "target_thres = 0.3\n",
    "print(np.min(np.abs(thres - target_thres)))\n",
    "print(precision[np.argmin(np.abs(thres - target_thres))], recall[np.argmin(np.abs(thres - target_thres))])\n",
    "print(f1_score(y_test==1, _y_pred > target_thres))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC curve\n",
    "rocs = []  # reset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_y_true = y_test\n",
    "_y_pred = y_dev_prob[0][:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rocs.append(roc_curve(_y_true, _y_pred, pos_label=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for roc in rocs[-3:]:\n",
    "    plt.plot(roc[0], roc[1])\n",
    "plt.legend([\"RNN-Attention\", \"CNN\", \"Logistic Regression\"], fontsize=14)\n",
    "plt.xlabel('False Positive Rate', fontsize=16)\n",
    "plt.ylabel('True Positive Rate', fontsize=16)\n",
    "plt.savefig('images/predict_present_roc_curve.pdf', dpi=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sensitivity    0.898551\n",
       "specificity    0.949580\n",
       "precision      0.837838\n",
       "accuracy       0.938111\n",
       "f1             0.867133\n",
       "dtype: float64"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "utils.my_classification_report(y_test==2, y_dev_prob[0][:, -1]>0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sensitivity               0.746032\n",
      "specificity               0.963115\n",
      "precision                 0.839286\n",
      "accuracy                  0.918567\n",
      "f1                        0.789916\n",
      "sensitivity CI    [0.6471, 0.8308]\n",
      "specificity CI    [0.9435, 0.9802]\n",
      "precision CI      [0.7586, 0.9138]\n",
      "accuracy CI       [0.8925, 0.9446]\n",
      "f1 CI             [0.7107, 0.8546]\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "model = \"TextRNN_field_aware\"\n",
    "output = utils.output_report(y_test==1, y_dev_prob[0][:, -1]>0.3)\n",
    "print(output)\n",
    "# output.to_csv(\"CI/\" + model +'.csv', header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sensitivity              -0.0793651\n",
      "specificity               0.0532787\n",
      "precision                  0.136583\n",
      "accuracy                  0.0260586\n",
      "f1                        0.0307919\n",
      "sensitivity CI    [-0.1965, 0.0402]\n",
      "specificity CI     [0.0163, 0.0887]\n",
      "precision CI       [0.0106, 0.2556]\n",
      "accuracy CI       [-0.0098, 0.0619]\n",
      "f1 CI             [-0.0649, 0.1226]\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "output = utils.output_report_diff(y1==1, y1_prob > 0.4, y_test==1, y_dev_prob[0][:, -1]>0.3)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TFIDF features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1220, 2346)\n",
      "(307, 2346)\n"
     ]
    }
   ],
   "source": [
    "ngram, min_count = 2, 10\n",
    "\n",
    "obj = utils.Df2TFIDF()\n",
    "obj.fit(df_train, ngram=ngram, min_count=min_count)\n",
    "output_train = obj.transform(df_train)\n",
    "output_test = obj.transform(df_test)\n",
    "# concatenate sparse matrices of all fields\n",
    "x_train = hstack([foo['bow_tfidf'] for foo in output_train.values()])\n",
    "x_test = hstack([foo['bow_tfidf'] for foo in output_test.values()])\n",
    "\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zyzzhaoyuzhe/virtualenvs/nlp3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# svm\n",
    "# clf = LinearSVC(C=1, loss='squared_hinge')\n",
    "# clf = SVC()\n",
    "clf = LogisticRegression(C=5)\n",
    "\n",
    "clf.fit(x_train, y_train)\n",
    "coef = clf.coef_.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sensitivity</th>\n",
       "      <th>specificity</th>\n",
       "      <th>precision</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>training</th>\n",
       "      <td>0.952055</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.988525</td>\n",
       "      <td>0.975439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>testing</th>\n",
       "      <td>0.825397</td>\n",
       "      <td>0.909836</td>\n",
       "      <td>0.702703</td>\n",
       "      <td>0.892508</td>\n",
       "      <td>0.759124</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          sensitivity  specificity  precision  accuracy        f1\n",
       "training     0.952055     1.000000   1.000000  0.988525  0.975439\n",
       "testing      0.825397     0.909836   0.702703  0.892508  0.759124"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_train_pred = clf.predict(x_train)\n",
    "y_test_pred = clf.predict(x_test)\n",
    "y_test_prob = clf.predict_proba(x_test)\n",
    "results = pd.concat([utils.my_classification_report(y_train==1, y_train_pred==1),\n",
    "                     utils.my_classification_report(y_test==1, y_test_prob[:,-1]>0.4),\n",
    "                     ], axis=1).transpose()\n",
    "results.index = ['training', 'testing']\n",
    "display(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename for Plottling PR/ROC Curve.\n",
    "_y_true = y_test==1\n",
    "_y_pred = y_test_prob[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x133573860>]"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAHn5JREFUeJzt3Xl83XWd7/HXJ3vSrG3SJVtTSrqEllLILSCKIBQLKnXU6y2IAhen4lhHxXFEYZBbnXG5M85Vh6tWHh3ADbno5XakI24oqBQSCi1dIXTL0iVt1mZP+rl/nGM8DW1z0p7kJL+8n4/HeeS3fHN+n29P8u4v399m7o6IiARLQrwLEBGR2FO4i4gEkMJdRCSAFO4iIgGkcBcRCSCFu4hIACncRUQCSOEuIhJACncRkQBKiteG8/PzvaysLF6bFxGZkF588cWj7l4wXLu4hXtZWRnV1dXx2ryIyIRkZvujaadhGRGRAFK4i4gEkMJdRCSAFO4iIgGkcBcRCaBhw93M1pvZETPbdpr1ZmbfNLMaM9tqZhfHvkwRERmJaPbcHwJWnGH99UB5+LUa+Pa5lyUiIudi2PPc3f0ZMys7Q5OVwCMeel7fJjPLNbNZ7n4wRjWepGpfE8++2jgabz0oOz2Z26+YQ2KCjep2RERGSywuYioCaiPm68LL3hDuZraa0N49paWlZ7Wxzfub+dbTNWf1vdH48yNlL587jQsKc0ZtOyIio2lMr1B193XAOoDKysqzejL3R946l4+8dW5M64r06x2H+fAj1Zw4MWqbEBEZdbE4W6YeKImYLw4vExGROIlFuG8APhQ+a+YyoHW0xttFRCQ6ww7LmNmPgauAfDOrA74AJAO4+3eAjcANQA3QCdw+WsWKiEh0ojlb5qZh1jvwsZhVJCIi50xXqIqIBFDc7ucuMh65O119A7R399Pe3U/p1AxSkrQPJBOPwl0CpbtvgLbuvsFwbu/uo60r9HVwvrs/ok3oa+T3DJz4y1m6t72pjPtvvCCOPRI5Owp3GZfcneM9/bR09tHc2UtTR+/gdHNnH80dvTR3hpaF1oWWd/UNnPF9zSAzNYnstGSy0pLISktiZnYa5dMzyRpcFvr6r796lZbO3jHqsUhsKdxlTHX3DdDY3sOR9m4a23vC0yd/bWzv4VhHD30Dp77OzQxy0pPJy0ghLyOZWTlpLJyVzdQpyeRmpJCdlkR2+slBnZWWTHZaElNSkkiI8rYS33t2Tyy7LjKmFO4SE+7OsY5eGlq6aGjpor6lm4aWLo6093CkrZvG46HQbu/uf8P3JhhMy0ylIDOV6dmpLJiZxbTM1MGwzstIOWk6Jz1Z9/0RGYbCXaLi7hw93su+Yx3sO9pBXXMoxBtau2ho6aa+pYve/pPv2ZCRksiM7DQKMlNZODObK8tTKcgKvaZn/WV62pRUhbVIjCncZdDQAN93rIN9xzrZd7SD/cc6Od7zl71uM5ielUpRbjoXFGZzXcUMCnPTw680inMzyE5PwkyhLRIPCvdJ6nhPP7sPtYdfbew61M7uw+20dPYNtklMMEry0inLn8J/KZtK2bQMZudPYc60KRTmpusUQZFxTOEecO7OgaZOtta1sutQG7sPtbPrUDt1zV2DbTJTk5g3I5PrF81i3oxMysIBXpSXTnKiAlxkIlK4B0xrZx8vHmji5QMtvFzXyta6lsG98aQE47yCKSwtzeOmZaXMn5HF/JlZFOela/hEJGAU7hPcodZunt97jKp9TVTtbWb34XYgdAbKvBlZvL1iJktKcrmwOIfyGZmkJiXGuWIRGQsK9wmmo6ef5/ce49nXjvKH147y2pHjAExJSeTi2Xm888JZVJZNZUlJDhkp+nhFJiv99k8ADS1d/GrHYX654xAv7G2ib8BJTUpg2Zyp/NfKYi4/L5+Fs7JI0vi4iIQp3MepvUc7eHJrA09tP8wr9a0AzC2Ywu1XzOHK8gIqy/JIS9YQy3jX1TsQvhagi4Ph6wEOhq8NaGjt4poF07nnHRXxLlMCSOE+jjR39PLzrQ38dHM9L9e2ALC0NJfPrljA8ooZnD89M84VSiR3p7G9h9rmzsErcg9GXJ17sLWL5ohTSyF0fUBBZiqFuem0dvaxaU9TnKqXoFO4x9mJE86zNUf54ab9PL37CH0DzoKZWXzu+gWsvKiImTlp8S5xUuvuO8HOg23UNnVyoKmT2qZOapu7Bqd7hlyVm52WNHgx19LS3MGLugpzQstmZKcNXh/w3x+qorG9Jx7dkklA4R4nHT39/GxzHQ/9aR+vN3aQn5nCrZeX8Z6Li6kozI53eQIkmPGL7Yf4xfZDg8syU5MomZrB3IIpXD2/gNKpGRRPzaA4N51ZuelkpupXSsaHqH4SzWwF8A0gEXjQ3b8yZP1sYD1QADQBt7h7XYxrDYQjbd1879k9PFpVS3t3P0uKc/hf/+0iblg8S1d8jjOfv2Ehrx1ppyQvg9KpoVduRrKuCZAJIZoHZCcCDwDLgTqgysw2uPuOiGb/DDzi7g+b2duALwMfHI2CJ6rmjl6+8/vXefi5ffQNOO9YPIvbrihjaUmuwmKcWl4xg+UVM8ZkW919A9Q1d1Hb3EldeOgnNATUyZG2Hr76vgu5ev70MalFgiGaPfdlQI277wEws0eBlUBkuFcAd4WnnwaeiGWRE1lbdx8PPruX9X/YS0dvP+++qIhPXlvO7GlT4l2ajAOvHm5n2T/+miNDxt5TkhIozkunMCedbfVtvHqoXeEuIxJNuBcBtRHzdcClQ9psAd5DaOjmr4AsM5vm7sdiUuUE5O78bHM9X3pyB82dfdyweCafunYe5TOy4l2ajBPXVcygq3eA4rx0SqZmUDI1nZK8DEqmZlCQmUpCgtHZ20/FfU/Fu1SZgGJ19OfvgH8zs9uAZ4B64A3POzOz1cBqgNLS0hhtevzZd7SDe554hT/WHOPi0ly+v3IRi4py4l2WjDOrlpWyallwfw8kvqIJ93qgJGK+OLxskLs3ENpzx8wygfe6e8vQN3L3dcA6gMrKylM/Q20C6xs4wbpn9vDN37xGSmICX3z3Ij6wrDTqx7qJiMRKNOFeBZSb2RxCob4KuDmygZnlA03ufgL4HKEzZyaV2qZOPvrDF9lW38b1i2Zy/40XMCNb56iLSHwMG+7u3m9ma4CnCJ0Kud7dt5vZWqDa3TcAVwFfNjMnNCzzsVGsedx59rVGPv7jlxg44XznlktYsWhmvEsSkUkuqjF3d98IbByy7L6I6ceBx2Nb2vjn7nz796/zz0/tpnx6Ft/94CWU5essGBGJP11Od5aO9/Tzd49t4RfbD/GuJYV89b2LdYtdERk3lEZnobWrj1sefJ4dB9u49x0LuePNc3QhkoiMKwr3EWrr7uND619g16E21n3wEq5ZODZXMIqIjIRuZjIC7d193Lr+BXY0tPK/P6BgF5HxS+EepY6efm7/9ypeqWvlWzddPGb3HBERORsalolCZ28/tz9UxUu1LXzrpqU61VFExj2F+zDcnc88vpXqfU18Y9VSblg8K94liQxydw62drPzYBs9/Sf08ymDFO7D+PELtTy59SB/v2I+71pSGO9yZBLr7O1n96F2dh1qZ9fBNnaGv7Z19w+2ef7z1+jKaAEU7me061Ab/+M/tvOW8nzuvHJuvMuRSexbv63hK7/YhYfvyDQlJZH5M7N415JCFszK5sCxDr737F76Bk6c+Y1k0lC4n0Znbz/3PLaN7PRkvv7+i3TzL4mLtKREblxSSN/ACRbMzGbBrCwWzsymOC/9pJ/Jx6prz/AuMhkp3E/jnzbu5PXG4/zgjkspyEqNdzkySSUkGN+8aWm8y5AJSKdCnsaWulbWXH0+V5yfH+9SRERGTOF+GsvKpvKJa8rjXYaIyFnRsMwQFYXZXL9oJv/wzgqSEvV/n4hMTAr3IQpz0/n2LZfEuwyRmGjq6GVbfSuv1LeyvaGVbfVtvG3BdO6/8YJ4lyajTOEuEiB/eO0oR9p72Fbfyrb6VhpauwfXlU7NoK27j5dq3/AETAkghbtIACQnhk6LvPtnrwBwXv4UKsumsqgom0WFOVxQmENORjK3rn+Blq6+eJYqY0ThLhIAyytm8rX3OWXTprBwVhZZacnxLkniTOEuEgCZqUm8v7JkzLbX1t3HjoY2uvsGuGr+9DHbrkQvqnA3sxXANwg9IPtBd//KkPWlwMNAbrjN3eHnrorIBNfc0cv2hja2NYTG8bc3tLH3aMfg+up7ryU/Uxf6jTfDhruZJQIPAMuBOqDKzDa4+46IZvcCj7n7t82sgtDDtMtGoV4RGUWN7T1sa2hle33ozJptDa3UNXcNri/OS2dRYQ7vu6SYg61d/GDTAd3PZpyKZs99GVDj7nsAzOxRYCUQGe4OZIenc4CGWBYpIrHX1NHLlroWttS2hM+uaeNQ21/OrpmTP4WLSnK55bLZLC7K4YLCbHIzUgbXP/rCgXiULVGKJtyLgMi7EtUBlw5pcz/wSzP7ODAFuPZUb2Rmq4HVAKWlpSOtVURi4PUjx7nya09zoKkTADOYW5DJ5XOncUFhNouLcqgozNZB2QkuVgdUbwIecvd/MbPLge+b2SJ3P+nvNXdfB6wDqKys9BhtW0SidGFxDq83HmdRUTYfuLSUJSW5LCrKITNV51YETTSfaD0QeRi+OLws0h3ACgB3f87M0oB84EgsihSR2Pj0dfP59HXz412GjIFowr0KKDezOYRCfRVw85A2B4BrgIfMbCGQBjTGslARGZ+21Lbwi22HeKW+lVfqWinKS+eh25fFu6xJb9hwd/d+M1sDPEXoNMf17r7dzNYC1e6+Afg08D0z+xShg6u3ubuGXUQC7M8PC7nzB5sByM9MJTEBDu3vPtO3yRiJaqAtfM76xiHL7ouY3gFcEdvSRGQ8W75wBp+/YQFz8jNZXJTDjOxU1v58B4+/WBfv0gRdoSoiZylvSgqr9WzhcUs3LBeRMdfdN4BGbkeX9txFZFR19w2w82AbW+ta2VLbwpa6FvYc7eAT15TzyWvnxbu8wFK4i0hM9Q84j1XVhq5+rWth96F2+gZCe+kFWaksKc6hvqWLw2068DqaFO4iEjMpiQl09Q3w9z/dSlZqEheW5PDht5zHkuIclpTkMjM7DTNj2T/+mo6eAf5Uc5Qtda1srWvh9cbjfOndi1k2Z2q8uxEICncRiZk73jKHxcU5LJyVzZxpUwZPlxzKDDZsaWDDltBtqApz0mho7WZbfavCPUYU7iISM9Oz0njnhYXDtvvsigXUN3dxYUkuFxblkGDGkrW/HIMKJw+Fu4iMufdcXHzSfGunHv0XazoVUkQkgBTuIiIBpHAXEQkghbuISAAp3EVEAkjhLiISQAp3EZEAUriLiASQwl1EJIAU7iIy7nT09POnmqP8pOoAfQMn4l3OhBTV7QfMbAXwDULPUH3Q3b8yZP2/AleHZzOA6e6eG8tCRST4/t/L9fx0cx07D7ZxIvwsj9nTpnDZedPiW9gENGy4m1ki8ACwHKgDqsxsQ/i5qQC4+6ci2n8cWDoKtYpIQKUmJ5CZmkTNkeNcVJrLmqvPJzU5kf/51G6OHu/ht7sO8+L+Zjbvb6Grb4CffOQyUpMS4132uBbNnvsyoMbd9wCY2aPASmDHadrfBHwhNuWJyGSQlpzIC/dcQ0piAkmJodHiqn1NAKz50UsAJCYYeRnJHD3eS2tXH9OzEjl6vIfN+5vJSU/m0mH27vsGTpCcOHlGoqMJ9yKgNmK+Drj0VA3NbDYwB/jtuZcmIpNJRsrJcXRBYTY3X1pKUW46l8zO48LiHH62uZ57n9jGPzyxjV2H2tl/rBMIPeGp6p5rB7/X3dlztIOXDrTw0oFmXjrQwu7D7dy1fB4fu/r8Me1XvMT6lr+rgMfdfeBUK81sNbAaoLS0NMabFpEgyUhJ4p/+avFJy/IzUwHYfKCFi0tzuXlZKc/vbaJqbxO/f7VxMMhfrm2htSt0G+Gs1CSWlOSSmGA0tHSNeT/iJZpwrwdKIuaLw8tOZRXwsdO9kbuvA9YBVFZW6tHnIjIib79gBlvuu47s9CTMQk95OtjazW93HeHW9S9gBvNnZHHD4pksLcljaWkucwsySUgwKr/0qzhXP7aiCfcqoNzM5hAK9VXAzUMbmdkCIA94LqYVioiEmRk5GcknLfvg5bMpzE1jUWEOi4tzyEpLPs13Ty7Dhru795vZGuApQqdCrnf37Wa2Fqh29w3hpquAR91de+QiMmbmFmQytyAz3mWMO1GNubv7RmDjkGX3DZm/P3ZliYjIuZg85wWJiEwiCncRkQBSuIuIBJDCXUQkgBTuIiIBpHAXEQmgWN9+QERkXGvp7GXzgWZe3N/M7kPH+czb5zN/Zla8y4o5hbuITBo/3VzHD58/AECCwQmHK+flK9xFRCaqGxbPorapk8qyqVxcmkdxXjpv+drT8S5r1CjcRWRSWLty0Unzx473xKmSsaEDqiIiAaRwFxEJIIW7iEgAKdxFRAJI4S4iEkAKdxGRAFK4i4gEkMJdRCSAogp3M1thZrvNrMbM7j5Nm/eb2Q4z225mP4ptmSIiMhLDXqFqZonAA8ByoA6oMrMN7r4jok058DngCndvNrPpo1WwiIgML5o992VAjbvvcfde4FFg5ZA2fw084O7NAO5+JLZliojISEQT7kVAbcR8XXhZpHnAPDP7o5ltMrMVsSpQRERGLlY3DksCyoGrgGLgGTNb7O4tkY3MbDWwGqC0tDRGmxYRkaGi2XOvB0oi5ovDyyLVARvcvc/d9wKvEgr7k7j7OnevdPfKgoKCs61ZRCTm3J09jcf53e4jnDjh8S7nnEWz514FlJvZHEKhvgq4eUibJ4CbgH83s3xCwzR7YlmoiMho+P3uRjbtOcYLe5s5Gr4N8P/9mzextDQvzpWdm2H33N29H1gDPAXsBB5z9+1mttbMbgw3ewo4ZmY7gKeBz7j7sdEqWkTkXKUkJZCUYPxm1xG21LbylvJ8br18NgDdfSfiXN25i2rM3d03AhuHLLsvYtqBu8IvEZFxLystmZ//7ZvJSkumKDcdgOdeP8bDz+2Pc2WxoScxiciktWBmdrxLGDW6/YCIyBn0D5ygo6c/3mWMmPbcRUSG2FrXQvW+Jl7Y18Tm/c048OK9y0lPSYx3aVFTuIuIhCUmGABf/s9dmMH8GVmcPyOLLbUtdPb2K9xFRCaipaW5fOFdFZROzaBy9lRyMpJ55Ll9bKltGfZ7xxuFu4hIWHJiArdfMSfeZcSEDqiKiASQwl1EJIAU7iIiAaRwFxEJIIW7iEgAKdxFRAJI4S4iEkAKdxGRAFK4i4iMQG//CV7c38TOg22Dy9yd/cc6aO3qi2NlJ9MVqiIiUfi3p2vY0dDGy7Ut9PSfID8zlTvfeh7V+5qp3h96itN1FTNY96HKeJcKaM9dROSMMlJC+8CPPLefrr4BPnDpbK6cV8DR4z186cmdbD/YypXl+RTlptPWrT13EZEJ4cYlhcwtmEL5jCwyU0OReaStm80HWlhSksOsnNBTnN7/3efiWeYbKNxFRM4gJSnhDQ/Lnp6dxopFM+NUUXSiGpYxsxVmttvMaszs7lOsv83MGs3s5fDrw7EvVUREojXsnruZJQIPAMuBOqDKzDa4+44hTX/i7mtGoUYRERmhaPbclwE17r7H3XuBR4GVo1uWiIici2jCvQiojZivCy8b6r1mttXMHjezkphUJyIiZyVWp0L+B1Dm7hcCvwIePlUjM1ttZtVmVt3Y2BijTYuIyFDRhHs9ELknXhxeNsjdj7l7T3j2QeCSU72Ru69z90p3rywoKDibekVEJArRhHsVUG5mc8wsBVgFbIhsYGazImZvBHbGrkQRERmpYc+Wcfd+M1sDPAUkAuvdfbuZrQWq3X0D8LdmdiPQDzQBt41izSIiMoyoLmJy943AxiHL7ouY/hzwudiWJiIiZ0v3lhERCSCFu4hIACncRUQCSOEuIhJACncRkQBSuIuIBJDCXUQkgBTuIiIBpHAXEQkghbuISAAp3EVEAkjhLiISQAp3EZEAUriLiASQwl1EJIAU7iIiAaRwFxEJIIW7iEgARRXuZrbCzHabWY2Z3X2Gdu81MzezytiVKCIiIzVsuJtZIvAAcD1QAdxkZhWnaJcFfAJ4PtZFiojIyESz574MqHH3Pe7eCzwKrDxFuy8CXwW6Y1ifiIichWjCvQiojZivCy8bZGYXAyXu/mQMaxMRkbN0zgdUzSwB+Drw6SjarjazajOrbmxsPNdNi4jIaUQT7vVAScR8cXjZn2UBi4Dfmdk+4DJgw6kOqrr7OnevdPfKgoKCs69aRETOKJpwrwLKzWyOmaUAq4ANf17p7q3unu/uZe5eBmwCbnT36lGpWEREhjVsuLt7P7AGeArYCTzm7tvNbK2Z3TjaBYqIyMglRdPI3TcCG4csu+80ba8697JERORc6ApVEZEAimrPXUREhtfVO8Bvdh7m5doWXq5tYXtDG3ctn8ctl80e81oU7iIiMZBgsKWulTseribBYP7MbNq6+njtcHtc6lG4i4jEwCevnceW2hYuKsllcXEOGSlJXLT2l3GrR+EuIhIDl503jcvOmxbvMgbpgKqISAAp3EVEAkjhLiISQAp3EZEAUriLiASQwl1EJIAU7iIiAaRwFxEJIIW7iEgAKdxFRAJI4S4iEkAKdxGRAFK4i4gEkMJdRCSAogp3M1thZrvNrMbM7j7F+jvN7BUze9nM/mBmFbEvVUREojVsuJtZIvAAcD1QAdx0ivD+kbsvdveLgK8BX495pSIiErVo9tyXATXuvsfde4FHgZWRDdy9LWJ2CuCxK1FEREYqmicxFQG1EfN1wKVDG5nZx4C7gBTgbTGpTkREzkrMDqi6+wPuPhf4LHDvqdqY2Wozqzaz6sbGxlhtWkREhogm3OuBkoj54vCy03kUePepVrj7OnevdPfKgoKC6KsUEZERiSbcq4ByM5tjZinAKmBDZAMzK4+YfQfwWuxKFBGRkRp2zN3d+81sDfAUkAisd/ftZrYWqHb3DcAaM7sW6AOagVtHs2gRkYliwJ2aI+3sONjOzoNt7DzYxm1vKuOq+dNHdbvRHFDF3TcCG4csuy9i+hMxrktEJBB+sOkAP9h0AICkBOP86Zl09Q6M+najCncRERm5j7+tnIaWLipmZbNwVjbnT88kJWlsbgygcBcRGSV3vHlO3Late8uIiASQwl1EJIAU7iIiAaRwFxEJIIW7iEgAKdxFRAJI4S4iEkAKdxGRADL3+DxXw8wagf1n+e35wNEYljMRqM+Tg/o8OZxLn2e7+7C31Y1buJ8LM6t298p41zGW1OfJQX2eHMaizxqWEREJIIW7iEgATdRwXxfvAuJAfZ4c1OfJYdT7PCHH3EVE5Mwm6p67iIicwbgOdzNbYWa7zazGzO4+xfpUM/tJeP3zZlY29lXGVhR9vsvMdpjZVjP7jZnNjkedsTRcnyPavdfM3Mwm/JkV0fTZzN4f/qy3m9mPxrrGWIviZ7vUzJ42s5fCP983xKPOWDGz9WZ2xMy2nWa9mdk3w/8eW83s4pgW4O7j8kXoea2vA+cBKcAWoGJIm78BvhOeXgX8JN51j0GfrwYywtMfnQx9DrfLAp4BNgGV8a57DD7ncuAlIC88Pz3edY9Bn9cBHw1PVwD74l33Ofb5SuBiYNtp1t8A/CdgwGXA87Hc/njec18G1Lj7HnfvBR4FVg5psxJ4ODz9OHCNmdkY1hhrw/bZ3Z92987w7CageIxrjLVoPmeALwJfBbrHsrhREk2f/xp4wN2bAdz9yBjXGGvR9NmB7PB0DtAwhvXFnLs/AzSdoclK4BEP2QTkmtmsWG1/PId7EVAbMV8XXnbKNu7eD7QC08akutERTZ8j3UHof/6JbNg+h/9cLXH3J8eysFEUzec8D5hnZn80s01mtmLMqhsd0fT5fuAWM6sDNgIfH5vS4makv+8jomeoTlBmdgtQCbw13rWMJjNLAL4O3BbnUsZaEqGhmasI/XX2jJktdveWuFY1um4CHnL3fzGzy4Hvm9kidz8R78ImovG8514PlETMF4eXnbKNmSUR+lPu2JhUNzqi6TNmdi1wD3Cju/eMUW2jZbg+ZwGLgN+Z2T5CY5MbJvhB1Wg+5zpgg7v3ufte4FVCYT9RRdPnO4DHANz9OSCN0D1Ygiqq3/ezNZ7DvQooN7M5ZpZC6IDphiFtNgC3hqffB/zWw0cqJqhh+2xmS4HvEgr2iT4OC8P02d1b3T3f3cvcvYzQcYYb3b06PuXGRDQ/208Q2mvHzPIJDdPsGcsiYyyaPh8ArgEws4WEwr1xTKscWxuAD4XPmrkMaHX3gzF793gfUR7maPMNhPZYXgfuCS9bS+iXG0If/v8BaoAXgPPiXfMY9PnXwGHg5fBrQ7xrHu0+D2n7Oyb42TJRfs5GaDhqB/AKsCreNY9BnyuAPxI6k+Zl4Lp413yO/f0xcBDoI/SX2B3AncCdEZ/xA+F/j1di/XOtK1RFRAJoPA/LiIjIWVK4i4gEkMJdRCSAFO4iIgGkcBcRCSCFu4hIACncRUQCSOEuIhJA/x+8xzo7fKjKnAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pr_curve = precision_recall_curve(_y_true, _y_pred, pos_label=1)\n",
    "plt.plot(pr_curve[1], pr_curve[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Diff CI\n",
    "y1 = y_test\n",
    "y1_prob = y_test_prob[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
